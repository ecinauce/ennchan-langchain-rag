{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882e73ab-bce8-4f19-9a0e-307699bcf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass # getpass.getpass()\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92aca6f6-0a15-420e-afe2-1421a2ba86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"WATSONX_APIKEY\"):\n",
    "#   os.environ[\"WATSONX_APIKEY\"] = \"4k4-fF2FdG68lyO8W2byjRs-HK9BlA4A01R7IOS9P7Np\"\n",
    "\n",
    "# from langchain_ibm import ChatWatsonx\n",
    "\n",
    "# llm = ChatWatsonx(\n",
    "#     model_id=\"meta-llama/llama-3-3-70b-instruct\", \n",
    "#     url=\"https://us-south.ml.cloud.ibm.com\", \n",
    "#     project_id=\"0580f65e-d9ba-4ada-bf7c-96a5c4aa9ec8\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b838cc-6cea-4db9-92ad-e7a2954bcf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files:   0%|                       | 0/6 [00:00<?, ?it/s]C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 4933.66 MB. The target location C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--microsoft--Phi-4-reasoning\\blobs only has 2940.06 MB free disk space.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 4902.24 MB. The target location C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--microsoft--Phi-4-reasoning\\blobs only has 2940.06 MB free disk space.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 4771.17 MB. The target location C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--microsoft--Phi-4-reasoning\\blobs only has 2940.06 MB free disk space.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 4954.69 MB. The target location C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--microsoft--Phi-4-reasoning\\blobs only has 2940.06 MB free disk space.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:799: UserWarning: Not enough free disk space to download the file. The expected file size is: 4986.12 MB. The target location C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--microsoft--Phi-4-reasoning\\blobs only has 2940.06 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-4-reasoning\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False,\n",
    "    ),\n",
    "    # model_kwargs={\"quantization_config\": quantization_config},\n",
    ")\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375d2fe4-d2ca-41e0-a278-8e30722552ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\NickEcuacion\\OneDrive - IBM\\Documents\\Sandbox\\ennchan-langchain-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NickEcuacion\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89785f7c-de10-4b80-82c8-50f3733c3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78c8a29-1a7d-4595-a8a4-cceef171df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://en.wikipedia.org/wiki/World_War_II\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"mw-content-container\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b90b39a-c050-46a7-964f-665802a6d2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Allies were a group of countries that opposed the Axis powers during World War II. The list of Allies included countries such as the United Kingdom, United States, Soviet Union, and Poland, among others. These countries worked together to counter the Axis powers, which consisted of Japan, Italy, and Germany, and their respective allies.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who were the Allies?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76128b13-1469-45c6-99e9-833042c9cd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
